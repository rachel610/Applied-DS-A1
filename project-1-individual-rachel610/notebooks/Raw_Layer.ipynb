{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries and Packages\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spark session\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Project 1\") \n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\") \n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in Taxi data\n",
    "sdf_yellow = spark.read.parquet('../data/raw/Yellow_raw')\n",
    "sdf_green = spark.read.parquet('../data/raw/Green_raw')\n",
    "\n",
    "# Record the number of instances\n",
    "nraw_y = sdf_yellow.count()\n",
    "nraw_g = sdf_green.count()\n",
    "\n",
    "(1 - 25705901/ (nraw_y+nraw_g))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in External dataset - related to accidents in NYC in 2020\n",
    "accidents_landing = spark.read.csv('../data/raw/NYC Accidents 2020.csv')\n",
    "\n",
    "# Only interested in accidents involving taxis\n",
    "accidents_raw = accidents_landing.filter((F.col('_c24')=='Taxi')|(F.col('_c25')=='Taxi')|\n",
    "                         (F.col('_c26')=='Taxi')|(F.col('_c27')=='Taxi')|(F.col('_c28')=='Taxi'))\n",
    "\n",
    "n_acc = accidents_raw.count()\n",
    "print(\"Number of rows in Taxi Accident dataframe:\", n_acc)\n",
    "\n",
    "\n",
    "# Defining Schemas\n",
    "accidents_raw_schema = accidents_raw.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking Consistency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure everything has consistent casing\n",
    "def consistent_casing(df):\n",
    "    consistent_col_casing = [F.col(col_name).alias(col_name.lower()) for col_name in df.columns]\n",
    "    new_df = df.select(*consistent_col_casing) # casefolding to lowercase\n",
    "    return(new_df)\n",
    "\n",
    "# Apply functions\n",
    "sdf_yellow = consistent_casing(sdf_yellow)\n",
    "sdf_green = consistent_casing(sdf_green)\n",
    "\n",
    "# Define Schemas\n",
    "sdf_schema_yellow = sdf_yellow.schema\n",
    "sdf_schema_green = sdf_green.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datatype Conversion\n",
    "def convert_datatype(df, column_name, datatype):\n",
    "    df = df.withColumn(\n",
    "    column_name,\n",
    "    F.col(column_name).cast(datatype)\n",
    ")\n",
    "    return(df)\n",
    "\n",
    "# Apply functions to each dataframe\n",
    "sdf_yellow = convert_datatype(sdf_yellow, 'pulocationid', 'int')\n",
    "sdf_yellow = convert_datatype(sdf_yellow, 'dolocationid', 'int')\n",
    "sdf_yellow = convert_datatype(sdf_yellow, 'vendorid', 'int')\n",
    "\n",
    "sdf_green = convert_datatype(sdf_green, 'pulocationid', 'int')\n",
    "sdf_green = convert_datatype(sdf_green, 'dolocationid', 'int')\n",
    "sdf_green = convert_datatype(sdf_green, 'vendorid', 'int')\n",
    "\n",
    "# Check Schema\n",
    "sdf_yellow.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datatype conversion\n",
    "\n",
    "def convert_datatype(df, column_name, datatype):\n",
    "    df = df.withColumn(\n",
    "    column_name,\n",
    "    F.col(column_name).cast(datatype)\n",
    ")\n",
    "    return(df)\n",
    "\n",
    "accidents_raw = convert_datatype(accidents_raw, '_c3', 'int')\n",
    "accidents_raw = convert_datatype(accidents_raw, '_c4', 'double')\n",
    "accidents_raw = convert_datatype(accidents_raw, '_c5', 'double')\n",
    "accidents_raw = convert_datatype(accidents_raw, '_c10', 'int')\n",
    "accidents_raw = convert_datatype(accidents_raw, '_c11', 'int')\n",
    "accidents_raw = convert_datatype(accidents_raw, '_c12', 'int')\n",
    "accidents_raw = convert_datatype(accidents_raw, '_c13', 'int')\n",
    "accidents_raw = convert_datatype(accidents_raw, '_c14', 'int')\n",
    "accidents_raw = convert_datatype(accidents_raw, '_c15', 'int')\n",
    "accidents_raw = convert_datatype(accidents_raw, '_c16', 'int')\n",
    "accidents_raw = convert_datatype(accidents_raw, '_c17', 'int')\n",
    "accidents_raw = convert_datatype(accidents_raw, '_c23', 'int')\n",
    "accidents_raw = convert_datatype(accidents_raw, '_c0', 'string')\n",
    "accidents_raw = convert_datatype(accidents_raw, '_c1', 'string')\n",
    "\n",
    "accidents_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature Removal and Renaming\n",
    "\n",
    "# Remove features that are not shared\n",
    "sdf_green.filter(F.col('ehail_fee')!= None) # There are no values in the ehail_fee (green) column \n",
    "sdf_green = sdf_green.drop('ehail_fee') # Delete 'ehail_fee' since it's an empty column not listed in the data dictionary\n",
    "sdf_green = sdf_green.drop('trip_type')\n",
    "sdf_yellow = sdf_yellow.drop('airport_fee')\n",
    "\n",
    "# According to the data dictionary, (lpep_dropoff_datetime, lpep_pickup_datetime) and \n",
    "# (tpep_dropoff_datetime, tpep_pickup_datetime) are equivalent and are compatible formating\n",
    "# So rename lpep (green) to tpep\n",
    "sdf_green = sdf_green.withColumnRenamed('lpep_dropoff_datetime', 'tpep_dropoff_datetime')\n",
    "sdf_green = sdf_green.withColumnRenamed('lpep_pickup_datetime', 'tpep_pickup_datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns in taxi df\n",
    "accidents_raw = accidents_raw.withColumnRenamed('_c0', 'crash_date')\n",
    "accidents_raw = accidents_raw.withColumnRenamed('_c1', 'crash_time')\n",
    "accidents_raw = accidents_raw.withColumnRenamed('_c2', 'borough')\n",
    "accidents_raw = accidents_raw.withColumnRenamed('_c3', 'zip_code')\n",
    "accidents_raw = accidents_raw.withColumnRenamed('_c4', 'latitude')\n",
    "accidents_raw = accidents_raw.withColumnRenamed('_c5', 'longitude')\n",
    "accidents_raw = accidents_raw.withColumnRenamed('_c6', 'location')\n",
    "accidents_raw = accidents_raw.withColumnRenamed('_c7', 'street_name')\n",
    "accidents_raw = accidents_raw.withColumnRenamed('_c8', 'cross_street_name')\n",
    "accidents_raw = accidents_raw.withColumnRenamed('_c9', 'off_street_name')\n",
    "accidents_raw = accidents_raw.withColumnRenamed('_c10', 'persons_injured')\n",
    "accidents_raw = accidents_raw.withColumnRenamed('_c11', 'persons_killed')\n",
    "accidents_raw = accidents_raw.withColumnRenamed('_c12', 'pedestrians_injured')\n",
    "accidents_raw = accidents_raw.withColumnRenamed('_c13', 'pedestrians_killed')\n",
    "accidents_raw = accidents_raw.withColumnRenamed('_c14', 'cyclists_injured')\n",
    "accidents_raw = accidents_raw.withColumnRenamed('_c15', 'cyclists_killed')\n",
    "accidents_raw = accidents_raw.withColumnRenamed('_c16', 'motorists_injured')\n",
    "accidents_raw = accidents_raw.withColumnRenamed('_c17', 'motorists_killed')\n",
    "accidents_raw = accidents_raw.withColumnRenamed('_c18', 'contributing_factor_vehicle1')\n",
    "accidents_raw = accidents_raw.withColumnRenamed('_c19', 'contributing_factor_vehicle2')\n",
    "accidents_raw = accidents_raw.withColumnRenamed('_c20', 'contributing_factor_vehicle3')\n",
    "accidents_raw = accidents_raw.withColumnRenamed('_c21', 'contributing_factor_vehicle4')\n",
    "accidents_raw = accidents_raw.withColumnRenamed('_c22', 'contributing_factor_vehicle5')\n",
    "accidents_raw = accidents_raw.withColumnRenamed('_c23', 'collision_id')\n",
    "accidents_raw = accidents_raw.withColumnRenamed('_c24', 'vehicle1_type')\n",
    "accidents_raw = accidents_raw.withColumnRenamed('_c25', 'vehicle2_type')\n",
    "accidents_raw = accidents_raw.withColumnRenamed('_c26', 'vehicle3_type')\n",
    "accidents_raw = accidents_raw.withColumnRenamed('_c27', 'vehicle4_type')\n",
    "accidents_raw = accidents_raw.withColumnRenamed('_c28', 'vehicle5_type')\n",
    "accidents_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraneous features for removal\n",
    "accidents_raw = accidents_raw.drop('collision_id')\n",
    "accidents_raw = accidents_raw.drop('off_street_name')\n",
    "accidents_raw = accidents_raw.drop('street_name')\n",
    "accidents_raw = accidents_raw.drop('cross_street_name')\n",
    "\n",
    "accidents_raw.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging Taxi Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create column to specify taxi colour before merging\n",
    "sdf_yellow = sdf_yellow.withColumn(\"taxi_colour\", lit('Y'))\n",
    "sdf_green = sdf_green.withColumn(\"taxi_colour\", lit('G'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort columns alphabetically before merging\n",
    "sdf_yellow = sdf_yellow.select(sorted(sdf_yellow.columns))\n",
    "sdf_green = sdf_green.select(sorted(sdf_green.columns))\n",
    "\n",
    "# Merge dataframes\n",
    "sdf_all = sdf_yellow.unionByName(sdf_green)\n",
    "n_landing = sdf_all.count()\n",
    "n_landing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove irrelevant features\n",
    "sdf_all = sdf_all.drop('extra')\n",
    "sdf_all = sdf_all.drop('fare_amount')\n",
    "sdf_all = sdf_all.drop('improvement_surcharge')\n",
    "sdf_all = sdf_all.drop('mta_tax')\n",
    "sdf_all = sdf_all.drop('payment_type')\n",
    "sdf_all = sdf_all.drop('tolls_amount')\n",
    "sdf_all = sdf_all.drop('tip_amount')\n",
    "sdf_all = sdf_all.drop('total_amount')\n",
    "sdf_all = sdf_all.drop('passenger_count')\n",
    "sdf_all = sdf_all.drop('ratecodeid')\n",
    "sdf_all = sdf_all.drop('store_and_fwd_flag')\n",
    "sdf_all = sdf_all.drop('vendorid')\n",
    "\n",
    "# Print Remaining number of features\n",
    "print(\"Number of Columns in Dataframe: \", len(sdf_all.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_all.columns # 6 columns retained from original dataset (taxi colour was added)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((accidents_raw.columns)) # 14 columns retained from original dataset, 2 columns added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validity Checking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete Records of 0 trip distance\n",
    "sdf_all = sdf_all.filter(F.col('trip_distance')>0)\n",
    "\n",
    "# Filter out unknown taxi zones\n",
    "sdf_all = sdf_all.filter((sdf_all.dolocationid != 264))\n",
    "sdf_all = sdf_all.filter((sdf_all.dolocationid != 265))\n",
    "sdf_all = sdf_all.filter((sdf_all.pulocationid != 264))\n",
    "sdf_all = sdf_all.filter((sdf_all.pulocationid != 265))\n",
    "\n",
    "n_raw = sdf_all.count()\n",
    "\n",
    "#sdf_all.count()\n",
    "print(n_landing - n_raw)\n",
    "print((n_landing - n_raw)/(nraw_y-nraw_g))\n",
    "\n",
    "# Print Schema\n",
    "sdf_all.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Relevant Columns from the Taxi Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define columns to sum\n",
    "fatalities = ['persons_killed', 'pedestrians_killed','cyclists_killed', 'motorists_killed']\n",
    "injuries = ['persons_injured', 'pedestrians_injured','cyclists_injured', 'motorists_injured']\n",
    "\n",
    "#create columns totaling the injuries and fatalties and harm\n",
    "accidents_raw = accidents_raw.withColumn('fatalities', F.expr('+'.join(fatalities)))\n",
    "accidents_raw = accidents_raw.withColumn('injuries', F.expr('+'.join(injuries)))\n",
    "accidents_raw = accidents_raw.withColumn('total_harmed', accidents_raw['injuries']+accidents_raw['fatalities'])\n",
    "\n",
    "# Create column summing number of vechicles involved in crash\n",
    "accidents_raw = accidents_raw.toPandas()\n",
    "vehicles = ['vehicle1_type', 'vehicle2_type', 'vehicle3_type', 'vehicle4_type', 'vehicle5_type']\n",
    "num_vehicles = accidents_raw[vehicles].notnull().sum(axis=1)\n",
    "accidents_raw['num_vehicles'] = num_vehicles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following features have been previously aggregated\n",
    "accidents_raw = accidents_raw.drop('persons_injured', axis = 1)\n",
    "accidents_raw = accidents_raw.drop('persons_killed', axis = 1)\n",
    "accidents_raw = accidents_raw.drop('pedestrians_injured', axis = 1)\n",
    "accidents_raw = accidents_raw.drop('pedestrians_killed', axis = 1)\n",
    "accidents_raw = accidents_raw.drop('cyclists_injured', axis = 1)\n",
    "accidents_raw = accidents_raw.drop('cyclists_killed', axis = 1)\n",
    "accidents_raw = accidents_raw.drop('motorists_injured', axis = 1)\n",
    "accidents_raw = accidents_raw.drop('motorists_killed', axis = 1)\n",
    "accidents_raw = accidents_raw.drop('vehicle1_type', axis = 1)\n",
    "accidents_raw = accidents_raw.drop('vehicle2_type', axis = 1)\n",
    "accidents_raw = accidents_raw.drop('vehicle3_type', axis = 1)\n",
    "accidents_raw = accidents_raw.drop('vehicle4_type', axis = 1)\n",
    "accidents_raw = accidents_raw.drop('vehicle5_type', axis = 1)\n",
    "\n",
    "# Convert back to spark df\n",
    "accidents_raw = spark.createDataFrame(accidents_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_all.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Updated files\n",
    "accidents_raw.write.parquet('../data/landing/accidents_landing', mode = 'overwrite')\n",
    "sdf_all.coalesce(1).write.format('parquet').mode('append').save('../data/landing/taxis_landing.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
